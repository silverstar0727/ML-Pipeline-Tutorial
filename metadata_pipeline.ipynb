{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "metadata_pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNYiWJoB75lplvSlQmszLbr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silverstar0727/ML-Pipeline-Tutorial/blob/main/metadata_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OWuiQ8ensT0"
      },
      "source": [
        "## 설치 및 라이브러리 임포트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5a9ZFLANh98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69f79775-ef53-4f7f-9098-d001b42ea6a6"
      },
      "source": [
        "# 해당 셀을 실행한 후에 반드시 \"런타임 다시시작\"을 해주세요\n",
        "!pip install -q kfp"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 235kB 7.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 12.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 15.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 32.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8MB 22.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 8.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 8.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 7.4MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 92kB 9.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 42.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 11.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.5MB/s \n",
            "\u001b[?25h  Building wheel for docstring-parser (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for kfp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for kfp-server-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for strip-hints (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: nbclient 0.5.3 has requirement jupyter-client>=6.1.5, but you'll have jupyter-client 5.3.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-cloud-bigquery 1.21.0 has requirement google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1, but you'll have google-resumable-media 1.3.1 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4tXVkJumpXc"
      },
      "source": [
        "from typing import NamedTuple\n",
        "import json \n",
        "\n",
        "import kfp\n",
        "from kfp import dsl\n",
        "from kfp.v2 import compiler\n",
        "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
        "                        OutputPath, component, ClassificationMetrics)\n",
        "from kfp.v2.google.client import AIPlatformClient"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuGyYnmcnz4X"
      },
      "source": [
        "## GCP 연결"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WimjuxN-BU1K"
      },
      "source": [
        "# gcp 연결\n",
        "from google.colab import auth as google_auth\n",
        "\n",
        "google_auth.authenticate_user()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWBrFnZ_dFy-"
      },
      "source": [
        "# 경로변수 설정\n",
        "아래 항목들을 본인 환경에 맞게 수정해주세요.\n",
        "\n",
        "* PROJECT_ID = <프로젝트 ID>\n",
        "* REGION = <리전>\n",
        "* BUCKET_NAME = <bucket 이름>\n",
        "* USER = <user 이름>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NFyqL4qfojZ"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "PROJECT_ID = 'mlops-210515'\n",
        "REGION = \"us-central1\"\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "BUCKET_NAME = \"gs://pipeline-129332\"\n",
        "\n",
        "USER = \"JeongMin-Do\"\n",
        "PIPELINE_ROOT = \"{}/pipeline_root/{}\".format(BUCKET_NAME, USER)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66wXmSVO-3jp"
      },
      "source": [
        "## Metadata pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm1E155of9po"
      },
      "source": [
        "@component\n",
        "def preprocess(\n",
        "    # An input parameter of type string.\n",
        "    message: str,\n",
        "    # Use Output to get a metadata-rich handle to the output artifact\n",
        "    # of type `Dataset`.\n",
        "    output_dataset_one: Output[Dataset],\n",
        "    # A locally accessible filepath for another output artifact of type\n",
        "    # `Dataset`.\n",
        "    output_dataset_two_path: OutputPath(\"Dataset\"),\n",
        "    # A locally accessible filepath for an output parameter of type string.\n",
        "    output_parameter_path: OutputPath(str),\n",
        "):\n",
        "    \"\"\"'Mock' preprocessing step.\n",
        "    Writes out the passed in message to the output \"Dataset\"s and the output message.\n",
        "    \"\"\"\n",
        "    output_dataset_one.metadata[\"hello\"] = \"there\"\n",
        "    # Use OutputArtifact.path to access a local file path for writing.\n",
        "    # One can also use OutputArtifact.uri to access the actual URI file path.\n",
        "    with open(output_dataset_one.path, \"w\") as f:\n",
        "        f.write(message)\n",
        "\n",
        "    # OutputPath is used to just pass the local file path of the output artifact\n",
        "    # to the function.\n",
        "    with open(output_dataset_two_path, \"w\") as f:\n",
        "        f.write(message)\n",
        "\n",
        "    with open(output_parameter_path, \"w\") as f:\n",
        "        f.write(message)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-nMcE43gCHE"
      },
      "source": [
        "@component(\n",
        "    base_image=\"python:3.9\",  # Use a different base image.\n",
        ")\n",
        "def train(\n",
        "    # An input parameter of type string.\n",
        "    message: str,\n",
        "    # Use InputPath to get a locally accessible path for the input artifact\n",
        "    # of type `Dataset`.\n",
        "    dataset_one_path: InputPath(\"Dataset\"),\n",
        "    # Use InputArtifact to get a metadata-rich handle to the input artifact\n",
        "    # of type `Dataset`.\n",
        "    dataset_two: Input[Dataset],\n",
        "    # Output artifact of type Model.\n",
        "    imported_dataset: Input[Dataset],\n",
        "    model: Output[Model],\n",
        "    # An input parameter of type int with a default value.\n",
        "    num_steps: int = 3,\n",
        "    # Use NamedTuple to return either artifacts or parameters.\n",
        "    # When returning artifacts like this, return the contents of\n",
        "    # the artifact. The assumption here is that this return value\n",
        "    # fits in memory.\n",
        ") -> NamedTuple(\n",
        "    \"Outputs\",\n",
        "    [\n",
        "        (\"output_message\", str),  # Return parameter.\n",
        "        (\"generic_artifact\", Artifact),  # Return generic Artifact.\n",
        "    ],\n",
        "):\n",
        "    \"\"\"'Mock' Training step.\n",
        "    Combines the contents of dataset_one and dataset_two into the\n",
        "    output Model.\n",
        "    Constructs a new output_message consisting of message repeated num_steps times.\n",
        "    \"\"\"\n",
        "\n",
        "    # Directly access the passed in GCS URI as a local file (uses GCSFuse).\n",
        "    with open(dataset_one_path, \"r\") as input_file:\n",
        "        dataset_one_contents = input_file.read()\n",
        "\n",
        "    # dataset_two is an Artifact handle. Use dataset_two.path to get a\n",
        "    # local file path (uses GCSFuse).\n",
        "    # Alternately, use dataset_two.uri to access the GCS URI directly.\n",
        "    with open(dataset_two.path, \"r\") as input_file:\n",
        "        dataset_two_contents = input_file.read()\n",
        "\n",
        "    with open(model.path, \"w\") as f:\n",
        "        f.write(\"My Model\")\n",
        "\n",
        "    with open(imported_dataset.path, \"r\") as f:\n",
        "        data = f.read()\n",
        "    print(\"Imported Dataset:\", data)\n",
        "\n",
        "    # Use model.get() to get a Model artifact, which has a .metadata dictionary\n",
        "    # to store arbitrary metadata for the output artifact. This metadata will be\n",
        "    # recorded in Managed Metadata and can be queried later. It will also show up\n",
        "    # in the UI.\n",
        "    model.metadata[\"accuracy\"] = 0.9\n",
        "    model.metadata[\"framework\"] = \"Tensorflow\"\n",
        "    model.metadata[\"time_to_train_in_seconds\"] = 257\n",
        "\n",
        "    artifact_contents = \"{}\\n{}\".format(dataset_one_contents, dataset_two_contents)\n",
        "    output_message = \" \".join([message for _ in range(num_steps)])\n",
        "    return (output_message, artifact_contents)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k_FhLWrgEsU"
      },
      "source": [
        "@component\n",
        "def read_artifact_input(\n",
        "    generic: Input[Artifact],\n",
        "):\n",
        "    with open(generic.path, \"r\") as input_file:\n",
        "        generic_contents = input_file.read()\n",
        "        print(f\"generic contents: {generic_contents}\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZWVDkJ9hyEf"
      },
      "source": [
        "@dsl.pipeline(\n",
        "    # Default pipeline root. You can override it when submitting the pipeline.\n",
        "    pipeline_root=PIPELINE_ROOT,\n",
        "    # A name for the pipeline. Use to determine the pipeline Context.\n",
        "    name=\"metadata-pipeline-v2\",\n",
        ")\n",
        "def pipeline(message: str):\n",
        "    importer = kfp.dsl.importer(\n",
        "        artifact_uri=\"gs://ml-pipeline-playground/shakespeare1.txt\",\n",
        "        artifact_class=Dataset,\n",
        "        reimport=False,\n",
        "    )\n",
        "    preprocess_task = preprocess(message=message)\n",
        "    train_task = train(\n",
        "        dataset_one=preprocess_task.outputs[\"output_dataset_one\"],\n",
        "        dataset_two=preprocess_task.outputs[\"output_dataset_two\"],\n",
        "        imported_dataset=importer.output,\n",
        "        message=preprocess_task.outputs[\"output_parameter\"],\n",
        "        num_steps=5,\n",
        "    )\n",
        "    read_task = read_artifact_input(  # noqa: F841\n",
        "        train_task.outputs[\"generic_artifact\"]\n",
        "    )"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "yvPvJw8lgHWX",
        "outputId": "ccc07dce-459a-4588-8029-0f921360965f"
      },
      "source": [
        "compiler.Compiler().compile(\n",
        "    pipeline_func = pipeline, package_path = \"metadata-pipeline.json\"\n",
        ")\n",
        "\n",
        "api_client = AIPlatformClient(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        ")\n",
        "\n",
        "response = api_client.create_run_from_job_spec(\n",
        "    job_spec_path=\"metadata-pipeline.json\", pipeline_root=PIPELINE_ROOT, parameter_values={\"message\": \"Hello, World\"},\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/metadata-pipeline-v2-20210703115750?project=mlops-210515\" target=\"_blank\" >here</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6-qanUKODhi"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}