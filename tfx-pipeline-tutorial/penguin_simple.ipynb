{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "penguin_simple.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silverstar0727/ML-Pipeline-Tutorial/blob/main/tfx-pipeline-tutorial/penguin_simple.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj3FNMh_7hug"
      },
      "source": [
        "Written by TensorFlow\n",
        "\n",
        "Translated & Modified by Jeongmin-Do"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as4OTe2ukSqm"
      },
      "source": [
        "!pip install -q --upgrade pip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZOYTt1RW4TK"
      },
      "source": [
        "### Install TFX\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyQtljP-qPHY"
      },
      "source": [
        "# 실행 후 런타임 다시시작을 해주세요\n",
        "!pip install -q tfx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDtLdSkvqPHe"
      },
      "source": [
        "### Set up variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcUseqJaE2XN"
      },
      "source": [
        "import os\n",
        "\n",
        "PIPELINE_NAME = \"penguin-simple\"\n",
        "\n",
        "PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)                        # 아티팩트가 저장될 경로\n",
        "METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')          # ML metadata 저장소로 SQLite DB 사용경로\n",
        "SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)                # 모델이 서빙될 경로\n",
        "\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F2SRwRLSYGa"
      },
      "source": [
        "### Prepare example data\n",
        "[Palmer Penguins dataset](https://allisonhorst.github.io/palmerpenguins/articles/intro.html)\n",
        "\n",
        "데이터셋 features\n",
        "- culmen_length_mm : 부리 길이\n",
        "- culmen_depth_mm : 부리 깊이\n",
        "- flipper_length_mm : 날개 길이\n",
        "- body_mass_g : 몸무게\n",
        "\n",
        "이미 모든 feature는 normalized가 되어있고, 종을 예측하는 분류문제이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fxMs6u86acP"
      },
      "source": [
        "import urllib.request\n",
        "import tempfile\n",
        "\n",
        "DATA_ROOT = tempfile.mkdtemp(prefix='tfx-data')  # 임시 디렉토리 생성.\n",
        "_data_url = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/data/penguins_processed.csv'\n",
        "_data_filepath = os.path.join(DATA_ROOT, \"data.csv\")\n",
        "urllib.request.urlretrieve(_data_url, _data_filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eSz28UDSnlG"
      },
      "source": [
        "# 파일 일부 확인\n",
        "!head {_data_filepath}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH6gizcpSwWV"
      },
      "source": [
        "## Create a pipeline\n",
        "\n",
        "- CsvExampleGen: data file을 TFRecord 파일로 변환.\n",
        "- Trainer: 모델을 별도의 파이썬 파일로 작성하고 이를 이용하여 Train 컴포넌트로 훈련을 진행.\n",
        "- Pusher: 훈련된 ML model을 파이프라인 외부로 배포하는 과정을 거침.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOjDv93eS5xV"
      },
      "source": [
        "### Write model training code\n",
        "\n",
        "Trainer 컴포넌트를 실행하기 위한 훈련 스크립트 작성. 해당 파이썬 스크립트는 run_fn 함수를 반드시 포함하고 있어야 하고, Trainer는 이 함수를 실행하는 방식으로 진행."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aES7Hv5QTDK3"
      },
      "source": [
        "_trainer_module_file = 'penguin_trainer.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gnc67uQNTDfW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2493638-d75c-4d71-b42a-f4e6a3ed47c7"
      },
      "source": [
        "%%writefile {_trainer_module_file}\n",
        "\n",
        "from typing import List\n",
        "from absl import logging\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow_transform.tf_metadata import schema_utils\n",
        "\n",
        "from tfx.components.trainer.executor import TrainerFnArgs\n",
        "from tfx.components.trainer.fn_args_utils import DataAccessor\n",
        "from tfx_bsl.tfxio import dataset_options\n",
        "from tensorflow_metadata.proto.v0 import schema_pb2\n",
        "\n",
        "_FEATURE_KEYS = [\n",
        "    'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n",
        "]\n",
        "_LABEL_KEY = 'species'\n",
        "\n",
        "_TRAIN_BATCH_SIZE = 20\n",
        "_EVAL_BATCH_SIZE = 10\n",
        "\n",
        "_FEATURE_SPEC = {\n",
        "    **{\n",
        "        feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)\n",
        "           for feature in _FEATURE_KEYS\n",
        "       },\n",
        "    _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n",
        "}\n",
        "\n",
        "\n",
        "def _input_fn(file_pattern: List[str],\n",
        "              data_accessor: DataAccessor,\n",
        "              schema: schema_pb2.Schema,\n",
        "              batch_size: int = 200) -> tf.data.Dataset:\n",
        "  return data_accessor.tf_dataset_factory(\n",
        "      file_pattern,\n",
        "      dataset_options.TensorFlowDatasetOptions(\n",
        "          batch_size=batch_size, label_key=_LABEL_KEY),\n",
        "      schema=schema).repeat()\n",
        "\n",
        "\n",
        "def _build_keras_model() -> tf.keras.Model:\n",
        "  inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]\n",
        "  d = keras.layers.concatenate(inputs)\n",
        "  for _ in range(2):\n",
        "    d = keras.layers.Dense(8, activation='relu')(d)\n",
        "  outputs = keras.layers.Dense(3)(d)\n",
        "\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(1e-2),\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "  model.summary(print_fn=logging.info)\n",
        "  return model\n",
        "\n",
        "\n",
        "# TFX Trainer는 이 함수를 호출.\n",
        "def run_fn(fn_args: TrainerFnArgs):\n",
        "  schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)\n",
        "\n",
        "  train_dataset = _input_fn(\n",
        "      fn_args.train_files,\n",
        "      fn_args.data_accessor,\n",
        "      schema,\n",
        "      batch_size=_TRAIN_BATCH_SIZE)\n",
        "  eval_dataset = _input_fn(\n",
        "      fn_args.eval_files,\n",
        "      fn_args.data_accessor,\n",
        "      schema,\n",
        "      batch_size=_EVAL_BATCH_SIZE)\n",
        "\n",
        "  model = _build_keras_model()\n",
        "  model.fit(\n",
        "      train_dataset,\n",
        "      steps_per_epoch=fn_args.train_steps,\n",
        "      validation_data=eval_dataset,\n",
        "      validation_steps=fn_args.eval_steps)\n",
        "  \n",
        "  model.save(fn_args.serving_model_dir, save_format='tf')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting penguin_trainer.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3OkNz3gTLwM"
      },
      "source": [
        "### Write a pipeline definition\n",
        "\n",
        "아래의 세 컴포넌트를 포함하는 파이프라인 함수 작성 \n",
        "* ExampleGen\n",
        "* Trainer\n",
        "* Pusher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M49yYVNBTPd4"
      },
      "source": [
        "from tfx.components import CsvExampleGen\n",
        "from tfx.components import Pusher\n",
        "from tfx.components import Trainer\n",
        "from tfx.components.trainer.executor import GenericExecutor\n",
        "from tfx.dsl.components.base import executor_spec\n",
        "from tfx.orchestration import metadata\n",
        "from tfx.orchestration import pipeline\n",
        "from tfx.proto import pusher_pb2\n",
        "from tfx.proto import trainer_pb2\n",
        "\n",
        "# pipeline.Pipeline을 반환하는 파이프라인 함수 작성\n",
        "def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
        "                     module_file: str, serving_model_dir: str,\n",
        "                     metadata_path: str) -> pipeline.Pipeline:\n",
        "  # ExampleGen\n",
        "  example_gen = CsvExampleGen(input_base=data_root)                             # input data\n",
        "\n",
        "  # Trainer\n",
        "  trainer = Trainer(\n",
        "      module_file=module_file,                                                  # 앞서 생성한 훈련 스크립트\n",
        "      custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),    # GenericExecutor를 사용한 로컬 훈련\n",
        "      examples=example_gen.outputs['examples'],                                 # input data(=Examples)\n",
        "      train_args=trainer_pb2.TrainArgs(num_steps=100),                          # train 인자\n",
        "      eval_args=trainer_pb2.EvalArgs(num_steps=5))                              # evaluation 인자\n",
        "\n",
        "  # Pusher\n",
        "  pusher = Pusher(\n",
        "      model=trainer.outputs['model'],                                           # trainer에서의 훈련된 모델을 사용\n",
        "      push_destination=pusher_pb2.PushDestination(\n",
        "          filesystem=pusher_pb2.PushDestination.Filesystem(\n",
        "              base_directory=serving_model_dir)))                               # \"serving_model/penguin-simple\"로 모델 서빙\n",
        "\n",
        "  # 파이프라인에 위 세 컴포넌트를 포함.\n",
        "  components = [\n",
        "      example_gen,\n",
        "      trainer,\n",
        "      pusher,\n",
        "  ]\n",
        "\n",
        "  return pipeline.Pipeline(\n",
        "      pipeline_name=pipeline_name,                                              # pipeline 이름 지정\n",
        "      pipeline_root=pipeline_root,                                              # pipeline 저장 경로 지정\n",
        "      metadata_connection_config=metadata.sqlite_metadata_connection_config(    # metadata 연결 구성\n",
        "          metadata_path),\n",
        "      components=components)                                                    # 컴포넌트 지정"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJbq07THU2GV"
      },
      "source": [
        "## Run the pipeline\n",
        "\n",
        "TFX의 local_dag_runner를 이용한 오케스트레이터 실행\n",
        "\n",
        "오케스트레이터로는 다음의 세 가지가 가능\n",
        "* Kubeflow\n",
        "* Local\n",
        "* Apache Airflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAtfOZTYWJu-"
      },
      "source": [
        "import os\n",
        "from tfx.orchestration.local import local_dag_runner\n",
        "\n",
        "local_dag_runner.LocalDagRunner().run(\n",
        "  _create_pipeline(\n",
        "      pipeline_name=PIPELINE_NAME,\n",
        "      pipeline_root=PIPELINE_ROOT,\n",
        "      data_root=DATA_ROOT,\n",
        "      module_file=_trainer_module_file,\n",
        "      serving_model_dir=SERVING_MODEL_DIR,\n",
        "      metadata_path=METADATA_PATH))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppERq0Mj6xvW"
      },
      "source": [
        "파이프라인이 성공적으로 완료되면 로그 끝에 \"INFO:absl:Component Pusher is finished.\"가 표시됨.\n",
        "\n",
        "Pusher를 통해 SERVING_MODEL_DIR로 서빙. Colab의 왼쪽 패널에 있는 파일 탐색기 또는 다음 명령을 사용하여 결과를 확인."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTHROkqX6yHx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbc03dba-c181-4278-d9e6-d59ef4be8f33"
      },
      "source": [
        "!find {SERVING_MODEL_DIR}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "serving_model/penguin-simple\n",
            "serving_model/penguin-simple/1619662320\n",
            "serving_model/penguin-simple/1619662320/assets\n",
            "serving_model/penguin-simple/1619662320/saved_model.pb\n",
            "serving_model/penguin-simple/1619662320/variables\n",
            "serving_model/penguin-simple/1619662320/variables/variables.data-00000-of-00001\n",
            "serving_model/penguin-simple/1619662320/variables/variables.index\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BejFnepEZXEn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}